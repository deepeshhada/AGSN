{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AGSNv2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepeshhada/AGSN/blob/master/AGSNv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCGCNUyKVqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import math\n",
        "import statistics \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.nn import Parameter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVkj8xk-jbmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc_Hx75kNwVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# choose a dataset from: CUB, SUN, AWA, AWA2, APY\n",
        "_dataset = \"CUB\"\n",
        "data_root = \"./drive/My Drive/Colab Data/Deep Learning/datasets/ZSL Datasets/\" + _dataset + \"/\"\n",
        "locations = [\"trainval_loc\", \"test_seen_loc\"]\n",
        "model_path = \"./drive/My Drive/Colab Data/Deep Learning/saved models/AGSN/\" + _dataset + \"/SN/\"\n",
        "pretrained = True\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 128\n",
        "latent_dim = 1024\n",
        "embed_dim = 312"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPj6eYTRrd3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels, class_embeddings):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.class_embeddings = class_embeddings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        return (self.features[index], label, class_embeddings[label])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmE2ATJX6MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load mat files\n",
        "res101 = io.loadmat(data_root + \"res101.mat\")\n",
        "att_splits = io.loadmat(data_root + \"att_splits.mat\")\n",
        "\n",
        "resnet_features = res101['features'].T\n",
        "class_labels = res101['labels']\n",
        "class_embeddings = att_splits['att'].T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB8Zw4NJUtt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_splits(loc, shuffle=False):\n",
        "    indices = att_splits[loc].reshape(-1) - 1\n",
        "    features = resnet_features[indices]\n",
        "    labels = class_labels[indices].reshape(-1) - 1\n",
        "\n",
        "    split = Dataset(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        class_embeddings=class_embeddings\n",
        "    )\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=split,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return split, dataloader\n",
        "\n",
        "\n",
        "train_set, trainloader = generate_splits(loc=locations[0], shuffle=True)\n",
        "seen_test_set, seen_testloader = generate_splits(loc=locations[0], shuffle=False)\n",
        "unseen_test_set, unseen_testloader = generate_splits(loc=locations[1], shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaAtDuK4i4k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unseen_labels = np.unique(unseen_test_set.labels)\n",
        "seen_labels = np.unique(train_set.labels)\n",
        "\n",
        "seen_y = torch.tensor(seen_labels, device=device).long()\n",
        "seen_cy = torch.tensor(class_embeddings[seen_labels], device=device).float()\n",
        "unseen_y = torch.tensor(unseen_labels, device=device).long()\n",
        "unseen_cy = torch.tensor(class_embeddings[unseen_labels], device=device).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPnhJAWrKVnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spectral Normalization\n",
        "\n",
        "def l2normalize(v, eps=1e-12):\n",
        "    return v / (v.norm() + eps)\n",
        "\n",
        "\n",
        "class SpectralNorm(nn.Module):\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\n",
        "        super(SpectralNorm, self).__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.power_iterations = power_iterations\n",
        "        if not self._made_params():\n",
        "            self._make_params()\n",
        "\n",
        "    def _update_u_v(self):\n",
        "        u = getattr(self.module, self.name + \"_u\")\n",
        "        v = getattr(self.module, self.name + \"_v\")\n",
        "        w = getattr(self.module, self.name + \"_bar\")\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        for _ in range(self.power_iterations):\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
        "\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
        "\n",
        "    def _made_params(self):\n",
        "        try:\n",
        "            u = getattr(self.module, self.name + \"_u\")\n",
        "            v = getattr(self.module, self.name + \"_v\")\n",
        "            w = getattr(self.module, self.name + \"_bar\")\n",
        "            return True\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "\n",
        "    def _make_params(self):\n",
        "        w = getattr(self.module, self.name)\n",
        "\n",
        "        height = w.data.shape[0]\n",
        "        width = w.view(height, -1).data.shape[1]\n",
        "\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
        "        u.data = l2normalize(u.data)\n",
        "        v.data = l2normalize(v.data)\n",
        "        w_bar = Parameter(w.data)\n",
        "\n",
        "        del self.module._parameters[self.name]\n",
        "\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
        "\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._update_u_v()\n",
        "        return self.module.forward(*args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fena8z4V-Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_initialize(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(0.0, 0.02)\n",
        "        module.bias.data.normal_(0.0, 0.02)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is0I5qID6Uzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the out_features of both the classifier and regressor are hardcoded for now.\n",
        "# TODO: make the out_features generic.\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=200, bias=True), # Earlier out_features set to 200 but changed to 150\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        return F.cross_entropy(pred, true)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.model = nn.Linear(in_features=2048, out_features=312, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        pred = F.normalize(input=pred, p=2, dim=1)\n",
        "        similarity_scores = torch.mm(pred, seen_cy.T) # batch * 150\n",
        "        loss = F.cross_entropy(similarity_scores, true)\n",
        "\n",
        "        true_embeddings = torch.Tensor(class_embeddings[torch.Tensor.cpu(true)]).to(device)\n",
        "        true_similarity = torch.bmm(pred.view(similarity_scores.size(0), 1, -1), true_embeddings.view(similarity_scores.size(0), -1, 1))\n",
        "\n",
        "        numerator = torch.exp(true_similarity.view(-1))\n",
        "        denominator = torch.sum(torch.exp(similarity_scores), dim=1)\n",
        "        loss_temp = -(numerator/denominator).log().mean()\n",
        "\n",
        "        return loss_temp\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Pb5HxI80U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            SpectralNorm(nn.Linear(in_features=624, out_features=2048, bias=True)),\n",
        "            nn.BatchNorm1d(num_features=2048),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=2048, out_features=2048),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBYnxnOEDN1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            SpectralNorm(nn.Linear(in_features=2360, out_features=4096, bias=True)),\n",
        "            nn.BatchNorm1d(num_features=4096),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=1),\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yssq9qh_M3VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "fc = Classifier().to(device)\n",
        "fr = Regressor().to(device)\n",
        "\n",
        "if pretrained:\n",
        "    G.load_state_dict(torch.load(model_path + \"Generator\"))\n",
        "    D.load_state_dict(torch.load(model_path + \"Discriminator\"))\n",
        "    fc.load_state_dict(torch.load(model_path + \"Classifier\"))\n",
        "    fr.load_state_dict(torch.load(model_path + \"Regressor\"))\n",
        "else:\n",
        "    G.weights_init()\n",
        "    D.weights_init()\n",
        "    fc.weights_init()\n",
        "    fr.weights_init()\n",
        "\n",
        "lr = 0.001\n",
        "gamma = 0.01\n",
        "beeta = 0.1\n",
        "lamda = 10\n",
        "max_epochs = 2000\n",
        "\n",
        "G_params = list(G.parameters()) + list(fc.parameters()) + list(fr.parameters())\n",
        "G_optimizer = optim.Adam(G_params, lr=lr, betas=(0.5, 0.999))\n",
        "D_optimizer = optim.Adam(D.parameters(), lr = lr, betas=(0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GYE6jAdXe89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not pretrained:\n",
        "    for epoch in range(max_epochs):\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            f, l, e = data\n",
        "            features, labels, embeddings = f.to(device).float(), l.to(device).long(), e.to(device).float()\n",
        "            b_size = embeddings.size(0)\n",
        "\n",
        "            for k in range(2):\n",
        "                D_optimizer.zero_grad()\n",
        "                for parameter in D.parameters(): #Weight Clip\n",
        "                    parameter.data.clamp_(-0.01, 0.01)\n",
        "\n",
        "                # train with real features\n",
        "                disc_input_real = torch.cat((features, embeddings), dim=1)\n",
        "                disc_score_real = D(disc_input_real)\n",
        "                disc_loss_real = torch.mean(disc_score_real)\n",
        "\n",
        "                noise = torch.randn(b_size, 312, device=device)\n",
        "                gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "                fake_features = G(gen_input)\n",
        "                disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "                disc_score_fake = D(disc_input_fake)\n",
        "                disc_loss_fake = torch.mean(disc_score_fake)\n",
        "                \n",
        "                disc_loss = torch.mean(disc_score_fake - disc_score_real)  # Still have to implement grad penalty\n",
        "                disc_loss.backward(retain_graph=True)\n",
        "                D_optimizer.step()\n",
        "\n",
        "            # Wasserstein (Weight clipping) Generator loss\n",
        "            with torch.autograd.set_detect_anomaly(True):\n",
        "                G_optimizer.zero_grad()\n",
        "                noise = torch.randn(b_size, embed_dim, device=device)\n",
        "                gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "                fake_features = G(gen_input)\n",
        "\n",
        "                align_cls = fc(fake_features)\n",
        "                cls_loss = fc.loss(labels, align_cls) # Computing classifier loss\n",
        "\n",
        "                align_reg = fr(fake_features)\n",
        "                reg_loss = fr.loss(labels, align_reg) # Computing Regressor loss\n",
        "\n",
        "                disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "                disc_score_fake = D(disc_input_fake)\n",
        "                disc_loss_fake = torch.mean(disc_score_fake)\n",
        "                gen_loss = beeta*(cls_loss + (gamma * reg_loss)) - disc_loss_fake\n",
        "                gen_loss.backward()\n",
        "                G_optimizer.step()\n",
        "\n",
        "        \n",
        "        if epoch == 0 or (epoch+1) % 200 == 0:\n",
        "            print(\"Epochs: %d/%d | Discriminator Loss = %f | Generator Loss: %f\" % \n",
        "                (epoch+1, max_epochs, disc_loss.item(), gen_loss.item()))\n",
        "            if os.path.exists(model_path + \"Generator\"):\n",
        "                os.remove(model_path + \"Generator\")\n",
        "            if os.path.exists(model_path + \"Discriminator\"):\n",
        "                os.remove(model_path + \"Discriminator\")\n",
        "            if os.path.exists(model_path + \"Classifier\"):\n",
        "                os.remove(model_path + \"Classifier\")\n",
        "            if os.path.exists(model_path + \"Regressor\"):\n",
        "                os.remove(model_path + \"Regressor\")\n",
        "            torch.save(G.state_dict(), model_path + \"Generator\")\n",
        "            torch.save(D.state_dict(), model_path + \"Discriminator\")\n",
        "            torch.save(fc.state_dict(), model_path + \"Classifier\")\n",
        "            torch.save(fr.state_dict(), model_path + \"Regressor\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqnsC2-RPd2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating features from the unseen classes using trained generator\n",
        "\n",
        "x_train = torch.tensor(train_set.features, device=device).float()\n",
        "y_train = torch.tensor(train_set.labels, device=device).long()\n",
        "\n",
        "k = -1 # current index\n",
        "for c_y in unseen_cy:\n",
        "    k += 1\n",
        "    embed = c_y.repeat(1, 100).view(100, -1)  # 100 X 312\n",
        "    lab = (unseen_y[k]).repeat(1, 100).view(100) # 100 labels\n",
        "    rand_noise = torch.randn(100, 312, device=device)  # generate 100 features\n",
        "    gen_inp = torch.cat((rand_noise, embed), dim=1)\n",
        "    generated = G(gen_inp)  # 100 X 2048 : 100 features generated \n",
        "    x_train = torch.cat((x_train, generated), dim=0)\n",
        "    y_train = torch.cat((y_train, lab), dim=0)\n",
        "\n",
        "# permute augmented dataset\n",
        "p = torch.randperm(y_train.shape[0])\n",
        "x = torch.empty_like(x_train)\n",
        "y = torch.empty_like(y_train)\n",
        "\n",
        "for i in range(x_train.shape[0]):\n",
        "    x[i] = x_train[p[i]]\n",
        "\n",
        "for i in range(y_train.shape[0]):\n",
        "    y[i] = y_train[p[i]]\n",
        "\n",
        "x_train = x\n",
        "y_train = y\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG-iG8Q4NqhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Final_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Final_Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            SpectralNorm(nn.Linear(2048, 4096)),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Linear(4096, 200),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    # computes 200*200 confusion matrix for relevant classes\n",
        "    \n",
        "    def compute_confusion_matrix(self, inputs, classes):\n",
        "        per_class_acc = 0.0\n",
        "        nb_classes = 200\n",
        "        confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "        return(confusion_matrix)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1CLf3GphBXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax_cls = Final_Classifier()\n",
        "softmax_cls = softmax_cls.to(device)\n",
        "softmax_cls.weights_init()\n",
        "\n",
        "num_iters = 150\n",
        "lr = 0.0009\n",
        "cls_optimizer = optim.Adam(softmax_cls.parameters(), lr=lr, betas=(0.5,0.999))\n",
        "\n",
        "for ep in range(num_iters):\n",
        "    cls_optimizer.zero_grad()\n",
        "    final_preds = softmax_cls(x_train)\n",
        "    final_loss = F.cross_entropy(final_preds, y_train)\n",
        "    final_loss.backward(retain_graph=True)\n",
        "    cls_optimizer.step()\n",
        "    \n",
        "    print(\"loss after \", ep + 1, \"iters \", final_loss.item())\n",
        "\n",
        "    if ep % 20 == 0:\n",
        "        if os.path.exists(model_path + \"Final_cls\"):\n",
        "            os.remove(model_path + \"Final_cls\")\n",
        "        torch.save(softmax_cls.state_dict(), model_path + \"Final_cls\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohbHr0wVT4kM",
        "colab_type": "code",
        "outputId": "a7761914-2651-4a7f-ff78-f9c36562704a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Final Test and accuracy computation\n",
        "# compute confusion matrix for seen and unseen classes separately\n",
        "\n",
        "x_unseen = torch.tensor(unseen_test_set.features, device=device).float()\n",
        "y_unseen = torch.tensor(unseen_test_set.labels, device=device).long()\n",
        "x_seen = torch.tensor(seen_test_set.features, device=device).float()\n",
        "y_seen = torch.tensor(seen_test_set.labels, device=device).long()\n",
        "\n",
        "cm_unseen = softmax_cls.compute_confusion_matrix(x_unseen, y_unseen)\n",
        "cm_seen = softmax_cls.compute_confusion_matrix(x_seen, y_seen)\n",
        "\n",
        "# compute per class accuracy matrix\n",
        "\n",
        "acc_mat_us = ((cm_unseen.diag()/cm_unseen.sum(1)))\n",
        "acc_mat_s = ((cm_seen.diag()/cm_seen.sum(1)))\n",
        "\n",
        "unseen_acc = []\n",
        "seen_acc = []\n",
        "\n",
        "# Remove Nan's from irrelevant classes\n",
        "\n",
        "for acc in acc_mat_us:\n",
        "    if not math.isnan(acc):\n",
        "        unseen_acc.append(acc)\n",
        "\n",
        "for acc in acc_mat_s:\n",
        "    if not math.isnan(acc):\n",
        "        seen_acc.append(acc)\n",
        "\n",
        "# Compute per class accuracy\n",
        "\n",
        "per_class_seen = np.mean(seen_acc)\n",
        "per_class_unseen = np.mean(unseen_acc)\n",
        "stacked_accuracies = [per_class_seen, per_class_unseen]\n",
        "harmonic_mean = statistics.harmonic_mean(stacked_accuracies)\n",
        "\n",
        "print(\"Unseen Class Accuracy:\", per_class_unseen*100)\n",
        "print(\"Seen Class Accuracy iS:\", per_class_seen*100)\n",
        "print(\"Harmonic Mean Accuracy for GZSL:\", harmonic_mean*100)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unseen Class Accuracy: 35.748291015625\n",
            "Seen Class Accuracy iS: 42.06629395484924\n",
            "Harmonic Mean Accuracy for GZSL: 38.650803543252835\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}