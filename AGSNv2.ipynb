{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AGSNv2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepeshhada/AGSN/blob/master/AGSNv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCGCNUyKVqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import math\n",
        "import statistics \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVkj8xk-jbmo",
        "colab_type": "code",
        "outputId": "d7ec028d-8935-4221-fa62-90143ec2202e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc_Hx75kNwVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# choose a dataset from: CUB, SUN, AWA, AWA2, APY\n",
        "_dataset = \"CUB\"\n",
        "data_root = \"./drive/My Drive/Colab Data/Deep Learning/datasets/ZSL Datasets/\" + _dataset + \"/\"\n",
        "model_path = \"./drive/My Drive/Colab Data/Deep Learning/saved models/AGSN/\" + _dataset + \"/\"\n",
        "pretrained = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h2ZfEmf8njj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 64\n",
        "latent_dim = 1024\n",
        "embed_dim = 312"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPj6eYTRrd3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels, class_embeddings):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.class_embeddings = class_embeddings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        return (self.features[index], label, class_embeddings[label])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmE2ATJX6MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load mat files\n",
        "res101 = io.loadmat(data_root + \"res101.mat\")\n",
        "att_splits = io.loadmat(data_root + \"att_splits.mat\")\n",
        "\n",
        "resnet_features = res101['features'].T\n",
        "class_labels = res101['labels']\n",
        "class_embeddings = att_splits['att'].T\n",
        "\n",
        "# print((att_splits['trainval_loc']).reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB8Zw4NJUtt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_splits(loc, shuffle=False):\n",
        "    indices = att_splits[loc].reshape(-1) - 1\n",
        "    features = resnet_features[indices]\n",
        "    labels = class_labels[indices].reshape(-1) - 1\n",
        "\n",
        "    split = Dataset(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        class_embeddings=class_embeddings\n",
        "    )\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=split,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return split, dataloader\n",
        "\n",
        "\n",
        "train_set, trainloader = generate_splits(loc='trainval_loc', shuffle=True)\n",
        "seen_test_set, seen_testloader = generate_splits(loc='test_seen_loc', shuffle=False)\n",
        "unseen_test_set, unseen_testloader = generate_splits(loc='test_unseen_loc', shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaAtDuK4i4k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unseen_labels = np.unique(unseen_test_set.labels)\n",
        "seen_labels = np.unique(train_set.labels)\n",
        "\n",
        "seen_y = torch.tensor(seen_labels, device=device).long()\n",
        "seen_cy = torch.tensor(class_embeddings[seen_labels], device=device).float()\n",
        "unseen_y = torch.tensor(unseen_labels, device=device).long()\n",
        "unseen_cy = torch.tensor(class_embeddings[unseen_labels], device=device).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fena8z4V-Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_initialize(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(0.0, 0.02)\n",
        "        module.bias.data.normal_(0.0, 0.02)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is0I5qID6Uzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the out_features of both the classifier and regressor are hardcoded for now.\n",
        "# TODO: make the out_features generic.\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=200, bias=True), # Earlier out_features set to 200 but changed to 150\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        return F.cross_entropy(pred, true)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.model = nn.Linear(in_features=2048, out_features=312, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        pred = F.normalize(input=pred, p=2, dim=1)\n",
        "        similarity_scores = torch.mm(pred, seen_cy.T) # batch * 150\n",
        "        loss = F.cross_entropy(similarity_scores, true)\n",
        "\n",
        "        true_embeddings = torch.Tensor(class_embeddings[torch.Tensor.cpu(true)]).to(device)\n",
        "        true_similarity = torch.bmm(pred.view(similarity_scores.size(0), 1, -1), true_embeddings.view(similarity_scores.size(0), -1, 1))\n",
        "\n",
        "        numerator = torch.exp(true_similarity.view(-1))\n",
        "        denominator = torch.sum(torch.exp(similarity_scores), dim=1)\n",
        "        loss_temp = -(numerator/denominator).log().mean()\n",
        "\n",
        "        return loss_temp\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Pb5HxI80U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=624, out_features=2048, bias=True),\n",
        "            nn.BatchNorm1d(num_features=2048),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=2048, out_features=2048),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBYnxnOEDN1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2360, out_features=4096, bias=True),\n",
        "            nn.BatchNorm1d(num_features=4096),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=1),\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yssq9qh_M3VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "fc = Classifier().to(device)\n",
        "fr = Regressor().to(device)\n",
        "\n",
        "G.weights_init()\n",
        "D.weights_init()\n",
        "fc.weights_init()\n",
        "fr.weights_init()\n",
        "\n",
        "lr = 0.001\n",
        "gamma = 0.01\n",
        "beeta = 0.1\n",
        "lamda = 10\n",
        "max_epochs = 2000\n",
        "\n",
        "G_params = list(G.parameters()) + list(fc.parameters()) + list(fr.parameters())\n",
        "G_optimizer = optim.Adam(G_params, lr=lr, betas=(0.5, 0.999))\n",
        "D_optimizer = optim.Adam(D.parameters(), lr = lr, betas=(0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GYE6jAdXe89",
        "colab_type": "code",
        "outputId": "e627c0f7-f473-446d-8926-6b51a2d0e26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for epoch in range(max_epochs):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        f, l, e = data\n",
        "        features, labels, embeddings = f.to(device).float(), l.to(device).long(), e.to(device).float()\n",
        "        b_size = embeddings.size(0)\n",
        "\n",
        "        for k in range(2):\n",
        "            D_optimizer.zero_grad()\n",
        "            for parameter in D.parameters(): #Weight Clip\n",
        "                parameter.data.clamp_(-0.01, 0.01)\n",
        "\n",
        "            # train with real features\n",
        "            disc_input_real = torch.cat((features, embeddings), dim=1)\n",
        "            disc_score_real = D(disc_input_real)\n",
        "            disc_loss_real = torch.mean(disc_score_real)\n",
        "\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = G(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = D(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            \n",
        "            disc_loss = torch.mean(disc_score_fake - disc_score_real)  # Still have to implement grad penalty\n",
        "            disc_loss.backward(retain_graph=True)\n",
        "            D_optimizer.step()\n",
        "\n",
        "        # Wasserstein Generator loss\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            G_optimizer.zero_grad()\n",
        "            noise = torch.randn(b_size, embed_dim, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = G(gen_input)\n",
        "\n",
        "            align_cls = fc(fake_features)\n",
        "            cls_loss = fc.loss(labels, align_cls) # Computing classifier loss\n",
        "\n",
        "            align_reg = fr(fake_features)\n",
        "            reg_loss = fr.loss(labels, align_reg) # Computing Regressor loss\n",
        "\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = D(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            gen_loss = beeta*(cls_loss + (gamma * reg_loss)) - disc_loss_fake\n",
        "            gen_loss.backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "    \n",
        "    if epoch == 0 or (epoch+1) % 200 == 0:\n",
        "        print(\"Epochs: %d/%d | Discriminator Loss = %f | Generator Loss: %f\" % \n",
        "            (epoch+1, max_epochs, disc_loss.item(), gen_loss.item()))\n",
        "        if os.path.exists(model_path + \"Generator\"):\n",
        "            os.remove(model_path + \"Generator\")\n",
        "        torch.save(G.state_dict(), model_path + \"Generator\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 1/2000 | Discriminator Loss = -0.028687 | Generator Loss: 0.500063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqnsC2-RPd2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating features from the unseen classes using trained generator\n",
        "\n",
        "x_train = torch.tensor(train_set.features, device=device).float()\n",
        "y_train = torch.tensor(train_set.labels, device=device).long()\n",
        "\n",
        "k = -1 # current index\n",
        "for c_y in unseen_cy:\n",
        "    k += 1\n",
        "    embed = c_y.repeat(1, 100).view(100, -1)  # 100 X 312\n",
        "    lab = (unseen_y[k]).repeat(1, 100).view(100) # 100 labels\n",
        "    rand_noise = torch.randn(100, 312, device=device)  # generate 100 features\n",
        "    gen_inp = torch.cat((rand_noise, embed), dim=1)\n",
        "    generated = G(gen_inp)  # 100 X 2048 : 100 features generated \n",
        "    x_train = torch.cat((x_train, generated), dim=0)\n",
        "    y_train = torch.cat((y_train, lab), dim=0)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "# print(torch.unique(y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG-iG8Q4NqhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Final_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Final_Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Linear(2048, 200),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    # computes 200*200 confusion matrix for relevant classes\n",
        "    \n",
        "    def compute_confusion_matrix(self, inputs, classes):\n",
        "        per_class_acc = 0.0\n",
        "        nb_classes = 200\n",
        "        confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "        return(confusion_matrix)\n",
        "\n",
        "    def compute_per_class_acc(self, x_test, test_label, nclass):\n",
        "        # acc_per_class = np.zeros(nclass)\n",
        "        # for i in range(0, nclass):\n",
        "        #     # idx = (test_label == i)\n",
        "        #     idx = (test_label == nclass[i])\n",
        "        #     acc_per_class[i] = np.sum(test_label[idx]==predicted_label[idx]) / np.sum(idx)\n",
        "        # return np.mean(acc_per_class)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted_label = self.model(x_test)\n",
        "\n",
        "        acc_per_class = np.zeros(nclass.shape)\n",
        "        for i in range(0, nclass.shape):\n",
        "            idx = (test_label == nclass[i])\n",
        "            acc_per_class[i] = np.sum(test_label[idx]==predicted_label[idx]) / np.sum(idx)\n",
        "        return np.mean(acc_per_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Fa9uE1wlyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax_cls = Final_Classifier()\n",
        "softmax_cls = softmax_cls.to(device)\n",
        "softmax_cls.weights_init()\n",
        "\n",
        "num_iters = 100\n",
        "lr = 0.0009\n",
        "cls_optimizer = optim.Adam(softmax_cls.parameters(), lr=lr, betas=(0.5,0.999))\n",
        "\n",
        "for ep in range(num_iters):\n",
        "    cls_optimizer.zero_grad()\n",
        "    final_preds = softmax_cls(x_train)\n",
        "    final_loss = F.cross_entropy(final_preds, y_train)\n",
        "    final_loss.backward(retain_graph=True)\n",
        "    cls_optimizer.step()\n",
        "    \n",
        "    print(\"loss after \", ep + 1, \"iters \", final_loss.item())\n",
        "\n",
        "    if ep % 20 == 0:\n",
        "        if os.path.exists(model_path + \"Final_cls\"):\n",
        "            os.remove(model_path + \"Final_cls\")\n",
        "        torch.save(G.state_dict(), model_path + \"Final_cls\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohbHr0wVT4kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Final Test and accuracy computation\n",
        "# compute confusion matrix for seen and unseen classes separately\n",
        "\n",
        "x_unseen = torch.tensor(unseen_test_set.features, device=device).float()\n",
        "y_unseen = torch.tensor(unseen_test_set.labels, device=device).long()\n",
        "x_seen = torch.tensor(seen_test_set.features, device=device).float()\n",
        "y_seen = torch.tensor(seen_test_set.labels, device=device).long()\n",
        "\n",
        "cm_unseen = softmax_cls.compute_confusion_matrix(x_unseen, y_unseen)\n",
        "cm_seen = softmax_cls.compute_confusion_matrix(x_seen, y_seen)\n",
        "\n",
        "# compute per class accuracy matrix\n",
        "\n",
        "acc_mat_us = ((cm_unseen.diag()/cm_unseen.sum(1)))\n",
        "acc_mat_s = ((cm_seen.diag()/cm_seen.sum(1)))\n",
        "\n",
        "unseen_acc = []\n",
        "seen_acc = []\n",
        "\n",
        "# Remove Nan's from irrelevant classes\n",
        "\n",
        "for acc in acc_mat_us:\n",
        "    if not math.isnan(acc):\n",
        "        unseen_acc.append(acc)\n",
        "\n",
        "for acc in acc_mat_s:\n",
        "    if not math.isnan(acc):\n",
        "        seen_acc.append(acc)\n",
        "\n",
        "# Compute per class accuracy\n",
        "\n",
        "per_class_seen = np.mean(seen_acc)\n",
        "per_class_unseen = np.mean(unseen_acc)\n",
        "l = [per_class_seen, per_class_unseen]\n",
        "\n",
        "print(\"Unseen Class Accuracy:\", per_class_unseen*100)\n",
        "print(\"Seen Class Accuracy iS:\", per_class_seen*100)\n",
        "# print(\"Harmonic Mean Accuracy for GZSL:\", statistics.harmonic_mean(l)*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5weQvecjiFnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_per_class_acc(x_test, test_label, nclass):\n",
        "    x = torch.tensor(x_test, device=device).float()\n",
        "    with torch.no_grad():\n",
        "        outputs = softmax_cls(x)\n",
        "        _, predicted_label = torch.max(outputs, 1)\n",
        "\n",
        "    print(torch.unique(predicted_label).shape)\n",
        "    # print(predicted_label.shape)\n",
        "\n",
        "    acc_per_class = np.zeros(nclass.shape)\n",
        "    print(test_label.shape)\n",
        "    # for i, data in enumerate(dataloader, 0):\n",
        "    #     f, l, e = data\n",
        "    #     features, labels = f.to(device).float(), l.to(device).long()\n",
        "    #     pred = softmax_cls(f)\n",
        "\n",
        "    for i in range(0, nclass.shape[0]):\n",
        "        idx = (test_label == nclass[i])\n",
        "        acc_per_class[i] = np.sum(test_label[idx]==predicted_label[idx]) / np.sum(idx)\n",
        "    return np.mean(acc_per_class)\n",
        "\n",
        "\n",
        "cm_u = compute_per_class_acc(unseen_test_set.features, unseen_test_set.labels, unseen_labels)\n",
        "cm_s = compute_per_class_acc(seen_test_set.features, seen_test_set.labels, seen_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}