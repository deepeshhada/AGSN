{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZSL - SABR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepeshhada/SABR/blob/master/train_jayant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCGCNUyKVqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVkj8xk-jbmo",
        "colab_type": "code",
        "outputId": "91ce49ca-9597-4611-9214-a9fe902cd2d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc_Hx75kNwVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Sz3S_oTgW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set dataset from: CUB, SUN, AWA, AWA2, APY\n",
        "_dataset = \"CUB\"\n",
        "data_root = \"./drive/My Drive/ZSL Datasets/\" + _dataset + \"/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPj6eYTRrd3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels, class_embeddings):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.class_embeddings = class_embeddings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        return (self.features[index], label, class_embeddings[label])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmE2ATJX6MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load mat files\n",
        "res101 = io.loadmat(data_root + \"res101.mat\")\n",
        "att_splits = io.loadmat(data_root + \"att_splits.mat\")\n",
        "\n",
        "resnet_features = res101['features'].T\n",
        "class_labels = res101['labels']\n",
        "class_embeddings = att_splits['att'].T\n",
        "\n",
        "# print((att_splits['trainval_loc']).reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB8Zw4NJUtt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8fb34fd-a30b-4778-9e60-49bd5b75719a"
      },
      "source": [
        "def generate_splits(loc, shuffle=False):\n",
        "    indices = att_splits[loc].reshape(-1) - 1\n",
        "    features = resnet_features[indices]\n",
        "    labels = class_labels[indices].reshape(-1) - 1\n",
        "\n",
        "    split = Dataset(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        class_embeddings=class_embeddings\n",
        "    )\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=split,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return split, dataloader\n",
        "\n",
        "\n",
        "train_set, trainloader = generate_splits(loc='trainval_loc', shuffle=True)\n",
        "seen_test_set, seen_testloader = generate_splits(loc='test_seen_loc', shuffle=False)\n",
        "unseen_test_set, unseen_testloader = generate_splits(loc='test_unseen_loc', shuffle=False)\n",
        "print(train_set.labels[0])"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaAtDuK4i4k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unseen_labels = np.unique(unseen_test_set.labels)\n",
        "seen_labels = np.unique(train_set.labels)\n",
        "\n",
        "unseen = class_embeddings[unseen_labels]\n",
        "seen = class_embeddings[seen_labels]\n",
        "\n",
        "unseen_cy = torch.tensor(unseen, device=device).float()\n",
        "seen_cy = torch.tensor(seen, device=device).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fena8z4V-Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_initialize(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(0.0, 0.02)\n",
        "        module.bias.data.normal_(0.0, 0.02)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is0I5qID6Uzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this in sync with the Generator\n",
        "# Generator class looks similar to the \"LatentTransform\" class\n",
        "# the out_features of both the classifier and regressor are hardcoded for now.\n",
        "# TODO: make the out_features generic.\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=150, bias=True), # Earlier out_features set to 200 but changed to 150\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        return F.cross_entropy(pred, true)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.model = nn.Linear(in_features=2048, out_features=312, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    # def loss(self, true, pred, b_size):   # to be checked for possible errors..\n",
        "    #     # true, pred --> batch_size * 312\n",
        "    #     a = F.normalize(true, p=2, dim=1, eps=1e-8).to(device)\n",
        "    #     b = F.normalize(pred, p=2, dim=1, eps=1e-8).to(device)\n",
        "    #     similarity_scores = torch.mm(a, b.T)  # batch * batch\n",
        "    #     arr = np.arange(0,b_size)\n",
        "    #     target = torch.tensor(arr).to(device).long()\n",
        "    #     loss = F.cross_entropy(similarity_scores, target)\n",
        "    #     return loss\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        similarity_scores = torch.mm(pred, seen_cy.T) # batch * 150\n",
        "        loss = F.cross_entropy(similarity_scores, true)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):  # removed c_y from signature\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "        # norm = torch.norm(input=x, p=2, dim=1).detach()\n",
        "        # x = x.div(norm.expand_as(x))\n",
        "        # return torch.bmm(x, c_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Pb5HxI80U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=624, out_features=2048, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=2048, out_features=2048),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBYnxnOEDN1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2360, out_features=4096, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=1),\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yssq9qh_M3VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_model = Generator()\n",
        "discriminator_model = Discriminator()\n",
        "classifier_model = Classifier()\n",
        "regressor_model = Regressor()\n",
        "\n",
        "generator_model = generator_model.to(device)\n",
        "discriminator_model = discriminator_model.to(device)\n",
        "classifier_model = classifier_model.to(device)\n",
        "regressor_model = regressor_model.to(device)\n",
        "\n",
        "generator_model.weights_init()\n",
        "discriminator_model.weights_init()\n",
        "classifier_model.weights_init()\n",
        "regressor_model.weights_init()\n",
        "\n",
        "learning_rate = 0.001\n",
        "gamma = 0.01\n",
        "beeta = 0.1\n",
        "lamda = 10\n",
        "num_epochs = 20\n",
        "\n",
        "train_params = list(generator_model.parameters()) + list(classifier_model.parameters()) + list(regressor_model.parameters())\n",
        "G_optimizer = optim.Adam(train_params, lr=learning_rate, betas=(0.5,0.999))\n",
        "D_optimizer = optim.Adam(discriminator_model.parameters(), lr = learning_rate, betas=(0.5,0.999))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GYE6jAdXe89",
        "colab_type": "code",
        "outputId": "c7cb0e0e-947b-4930-e53b-afc203c253cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for ep in range(5):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        f, l, e = data\n",
        "        features, labels, embeddings = f.to(device).float(), l.to(device).long(), e.to(device).float()\n",
        "        discriminator_model.zero_grad()\n",
        "        b_size = embeddings.shape[0]\n",
        "        noise = torch.randn(b_size, 312, device=device)\n",
        "        gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "        fake_features = generator_model(gen_input)\n",
        "        align_cls = classifier_model(fake_features)\n",
        "        cls_loss = classifier_model.loss(labels, align_cls)  # Computing classifier loss\n",
        "        # print(cls_loss)\n",
        "        align_reg = regressor_model(fake_features)\n",
        "        reg_loss = regressor_model.loss(labels, align_reg) # Computing Regressor loss\n",
        "        # print(reg_loss)\n",
        "\n",
        "        # Discriminator Loss\n",
        "\n",
        "        for k in range(2):\n",
        "            # alpha = torch.rand((batch_size, 1)).to(device) # Random [0,1) from uniform dist.\n",
        "            # interpolate = alpha*features + (1 - alpha)*fake_features   # b_size * 2048\n",
        "\n",
        "            # train with real features\n",
        "            disc_input_real = torch.cat((features, embeddings), dim=1)\n",
        "            disc_score_real = discriminator_model(disc_input_real)\n",
        "            disc_loss_real = torch.mean(disc_score_real)\n",
        "\n",
        "            # train with fake features\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = generator_model(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = discriminator_model(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            \n",
        "            disc_loss_total = torch.mean(disc_score_fake - disc_score_real)  # Still have to implement grad penalty\n",
        "            # print(disc_loss_total)\n",
        "            disc_loss_total.backward(retain_graph=True)\n",
        "            D_optimizer.step()\n",
        "        \n",
        "\n",
        "        # Wasserstein Generator loss\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            generator_model.zero_grad()\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = generator_model(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = discriminator_model(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            gen_loss1 = -disc_loss_fake\n",
        "            gen_loss = gen_loss1 + beeta *(cls_loss + (gamma * reg_loss))\n",
        "            # print(gen_loss)\n",
        "            gen_loss.backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "    print(\"discriminator loss after\", ep+1, \" iterations\", disc_loss_total.item())\n",
        "    print(\"generator loss after\", ep+1, \" iterations\", gen_loss.item())\n",
        "    print()\n"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "discriminator loss after 1  iterations -4350.923828125\n",
            "generator loss after 1  iterations 5870.96240234375\n",
            "\n",
            "discriminator loss after 2  iterations -708.8563232421875\n",
            "generator loss after 2  iterations 1800.555908203125\n",
            "\n",
            "discriminator loss after 3  iterations -63.56293869018555\n",
            "generator loss after 3  iterations 2297.301025390625\n",
            "\n",
            "discriminator loss after 4  iterations -464.8307800292969\n",
            "generator loss after 4  iterations 2043.713623046875\n",
            "\n",
            "discriminator loss after 5  iterations -1525.1744384765625\n",
            "generator loss after 5  iterations 2483.531494140625\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqnsC2-RPd2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f4a4593c-30b9-4a64-d408-4e57b570cab8"
      },
      "source": [
        "# generating features from the unseen classes using trained generator\n",
        "\n",
        "x_train = torch.tensor(train_set.features, device=device).float()\n",
        "y_train = torch.tensor(train_set.labels, device=device).long()\n",
        "\n",
        "k = -1 # current index\n",
        "for c_y in unseen_cy:\n",
        "    k += 1\n",
        "    embed = c_y.repeat(1, 100).view(100, -1)  # 100 *312\n",
        "    lab = (y_train[k]).repeat(1, 100).view(100) # 100 labels\n",
        "    rand_noise = torch.randn(100, 312, device=device)  # generate 100 features\n",
        "    gen_inp = torch.cat((rand_noise, embed), dim=1)\n",
        "    generated = generator_model(gen_inp)  # 100 * 2048 : 100 features generated \n",
        "    x_train = torch.cat((x_train, generated), dim=0)\n",
        "    y_train = torch.cat((y_train, lab), dim=0)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([12057, 2048])\n",
            "torch.Size([12057])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG-iG8Q4NqhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Final_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Final_Classifier, self).__init__()\n",
        "        model = nn.Sequential(\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Linear(2048, 200),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}