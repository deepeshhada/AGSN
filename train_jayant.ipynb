{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZSL - SABR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepeshhada/SABR/blob/master/train_jayant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCGCNUyKVqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import math\n",
        "import statistics \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVkj8xk-jbmo",
        "colab_type": "code",
        "outputId": "3fb899f3-58d3-4829-9e2f-02bc314d3b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc_Hx75kNwVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Sz3S_oTgW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set dataset from: CUB, SUN, AWA, AWA2, APY\n",
        "_dataset = \"CUB\"\n",
        "data_root = \"./drive/My Drive/ZSL Datasets/\" + _dataset + \"/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPj6eYTRrd3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels, class_embeddings):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.class_embeddings = class_embeddings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        return (self.features[index], label, class_embeddings[label])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmE2ATJX6MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load mat files\n",
        "res101 = io.loadmat(data_root + \"res101.mat\")\n",
        "att_splits = io.loadmat(data_root + \"att_splits.mat\")\n",
        "\n",
        "resnet_features = res101['features'].T\n",
        "class_labels = res101['labels']\n",
        "class_embeddings = att_splits['att'].T\n",
        "\n",
        "# print((att_splits['trainval_loc']).reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB8Zw4NJUtt6",
        "colab_type": "code",
        "outputId": "0083841f-8678-4b3d-ec80-241a51a49c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def generate_splits(loc, shuffle=False):\n",
        "    indices = att_splits[loc].reshape(-1) - 1\n",
        "    features = resnet_features[indices]\n",
        "    labels = class_labels[indices].reshape(-1) - 1\n",
        "\n",
        "    split = Dataset(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        class_embeddings=class_embeddings\n",
        "    )\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=split,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return split, dataloader\n",
        "\n",
        "\n",
        "train_set, trainloader = generate_splits(loc='trainval_loc', shuffle=True)\n",
        "seen_test_set, seen_testloader = generate_splits(loc='test_seen_loc', shuffle=False)\n",
        "unseen_test_set, unseen_testloader = generate_splits(loc='test_unseen_loc', shuffle=False)\n",
        "print(train_set.labels[0])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaAtDuK4i4k0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "556fc77a-ee35-47b9-cdfe-8c25e72c0548"
      },
      "source": [
        "unseen_labels = np.unique(unseen_test_set.labels)\n",
        "seen_labels = np.unique(train_set.labels)\n",
        "\n",
        "unseen = class_embeddings[unseen_labels]\n",
        "seen = class_embeddings[seen_labels]\n",
        "\n",
        "unseen_cy = torch.tensor(unseen, device=device).float()\n",
        "seen_cy = torch.tensor(seen, device=device).float()\n",
        "unseen_y = torch.tensor(unseen_labels, device=device).long()\n",
        "seen_y = torch.tensor(seen_labels, device=device).long()\n",
        "print(unseen_y)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  6,  18,  20,  28,  33,  35,  49,  55,  61,  67,  68,  71,  78,  79,\n",
            "         86,  87,  90,  94,  97,  99, 103, 107, 115, 119, 121, 123, 124, 128,\n",
            "        138, 140, 141, 149, 151, 156, 158, 159, 165, 166, 170, 173, 175, 178,\n",
            "        181, 184, 186, 188, 190, 191, 192, 194], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fena8z4V-Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_initialize(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(0.0, 0.02)\n",
        "        module.bias.data.normal_(0.0, 0.02)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is0I5qID6Uzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this in sync with the Generator\n",
        "# Generator class looks similar to the \"LatentTransform\" class\n",
        "# the out_features of both the classifier and regressor are hardcoded for now.\n",
        "# TODO: make the out_features generic.\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=150, bias=True), # Earlier out_features set to 200 but changed to 150\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        return F.cross_entropy(pred, true)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.model = nn.Linear(in_features=2048, out_features=312, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        similarity_scores = torch.mm(pred, seen_cy.T) # batch * 150\n",
        "        loss = F.cross_entropy(similarity_scores, true)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):  # removed c_y from signature\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "        # norm = torch.norm(input=x, p=2, dim=1).detach()\n",
        "        # x = x.div(norm.expand_as(x))\n",
        "        # return torch.bmm(x, c_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Pb5HxI80U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=624, out_features=2048, bias=True),\n",
        "            nn.BatchNorm1d(num_features=2048),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=2048, out_features=2048),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBYnxnOEDN1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2360, out_features=4096, bias=True),\n",
        "            nn.BatchNorm1d(num_features=4096),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=1),\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yssq9qh_M3VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_model = Generator()\n",
        "discriminator_model = Discriminator()\n",
        "classifier_model = Classifier()\n",
        "regressor_model = Regressor()\n",
        "\n",
        "generator_model = generator_model.to(device)\n",
        "discriminator_model = discriminator_model.to(device)\n",
        "classifier_model = classifier_model.to(device)\n",
        "regressor_model = regressor_model.to(device)\n",
        "\n",
        "generator_model.weights_init()\n",
        "discriminator_model.weights_init()\n",
        "classifier_model.weights_init()\n",
        "regressor_model.weights_init()\n",
        "\n",
        "learning_rate = 0.001\n",
        "gamma = 0.01\n",
        "beeta = 0.1\n",
        "lamda = 10\n",
        "num_epochs = 20\n",
        "\n",
        "train_params = list(generator_model.parameters()) + list(classifier_model.parameters()) + list(regressor_model.parameters())\n",
        "G_optimizer = optim.Adam(train_params, lr=learning_rate, betas=(0.5,0.999))\n",
        "D_optimizer = optim.Adam(discriminator_model.parameters(), lr = learning_rate, betas=(0.5,0.999))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKIC4Bdl0i7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Computation of Gradien Penalty for WGAN-GP\n",
        "\n",
        "def compute_gradient_penalty(netD, real_data, fake_data):\n",
        "    batch = fake_data.shape[0]\n",
        "    alpha = torch.rand(batch, 1).to(device)  #\n",
        "\n",
        "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GYE6jAdXe89",
        "colab_type": "code",
        "outputId": "9f19f821-2ba3-446c-ad4e-9565af7458e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "for ep in range(50):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        f, l, e = data\n",
        "        features, labels, embeddings = f.to(device).float(), l.to(device).long(), e.to(device).float()\n",
        "        discriminator_model.zero_grad()\n",
        "        b_size = embeddings.shape[0]\n",
        "        noise = torch.randn(b_size, 312, device=device)\n",
        "        gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "        fake_features = generator_model(gen_input)\n",
        "        align_cls = classifier_model(fake_features)\n",
        "        cls_loss = classifier_model.loss(labels, align_cls)  # Computing classifier loss\n",
        "        # print(cls_loss)\n",
        "        align_reg = regressor_model(fake_features)\n",
        "        reg_loss = regressor_model.loss(labels, align_reg) # Computing Regressor loss\n",
        "        # print(reg_loss)\n",
        "\n",
        "        # Discriminator Loss\n",
        "\n",
        "        for k in range(2):\n",
        "            \n",
        "            for parameter in discriminator_model.parameters(): #Weight Clip\n",
        "                parameter.data.clamp_(-0.01, 0.01)\n",
        "\n",
        "            # train with real features\n",
        "            disc_input_real = torch.cat((features, embeddings), dim=1)\n",
        "            disc_score_real = discriminator_model(disc_input_real)\n",
        "            disc_loss_real = torch.mean(disc_score_real)\n",
        "\n",
        "            # train with fake features\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = generator_model(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = discriminator_model(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            \n",
        "            disc_loss_total = torch.mean(disc_score_fake - disc_score_real)  # Still have to implement grad penalty\n",
        "            # print(disc_loss_total)\n",
        "            disc_loss_total.backward(retain_graph=True)\n",
        "            D_optimizer.step()\n",
        "        \n",
        "\n",
        "        # Wasserstein Generator loss\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            generator_model.zero_grad()\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = generator_model(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = discriminator_model(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            gen_loss1 = -disc_loss_fake\n",
        "            gen_loss = gen_loss1 + beeta *(cls_loss + (gamma * reg_loss))\n",
        "            # print(gen_loss)\n",
        "            gen_loss.backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "    print(\"discriminator loss after\", ep+1, \" iterations\", disc_loss_total.item())\n",
        "    print(\"generator loss after\", ep+1, \" iterations\", gen_loss.item())\n",
        "    print()\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "discriminator loss after 37  iterations -0.04900946095585823\n",
            "generator loss after 37  iterations 0.517959713935852\n",
            "\n",
            "discriminator loss after 38  iterations -0.05648840218782425\n",
            "generator loss after 38  iterations 0.4985438585281372\n",
            "\n",
            "discriminator loss after 39  iterations -0.03151519224047661\n",
            "generator loss after 39  iterations 0.4791286885738373\n",
            "\n",
            "discriminator loss after 40  iterations -0.05288206785917282\n",
            "generator loss after 40  iterations 0.5154538750648499\n",
            "\n",
            "discriminator loss after 41  iterations -0.05131931230425835\n",
            "generator loss after 41  iterations 0.4680006504058838\n",
            "\n",
            "discriminator loss after 42  iterations -0.05648824945092201\n",
            "generator loss after 42  iterations 0.5045775175094604\n",
            "\n",
            "discriminator loss after 43  iterations -0.055108435451984406\n",
            "generator loss after 43  iterations 0.5035054087638855\n",
            "\n",
            "discriminator loss after 44  iterations -0.06063137203454971\n",
            "generator loss after 44  iterations 0.4975305497646332\n",
            "\n",
            "discriminator loss after 45  iterations -0.03621521592140198\n",
            "generator loss after 45  iterations 0.48894235491752625\n",
            "\n",
            "discriminator loss after 46  iterations -0.04326020926237106\n",
            "generator loss after 46  iterations 0.5019652843475342\n",
            "\n",
            "discriminator loss after 47  iterations -0.031111955642700195\n",
            "generator loss after 47  iterations 0.4647940695285797\n",
            "\n",
            "discriminator loss after 48  iterations -0.05060533061623573\n",
            "generator loss after 48  iterations 1.8394742012023926\n",
            "\n",
            "discriminator loss after 49  iterations -0.032364875078201294\n",
            "generator loss after 49  iterations 0.4903542697429657\n",
            "\n",
            "discriminator loss after 50  iterations -0.02170037478208542\n",
            "generator loss after 50  iterations 0.48543766140937805\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqnsC2-RPd2N",
        "colab_type": "code",
        "outputId": "22122c8a-d07a-4363-a71a-2984564f37a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# generating features from the unseen classes using trained generator\n",
        "\n",
        "x_train = torch.tensor(train_set.features, device=device).float()\n",
        "y_train = torch.tensor(train_set.labels, device=device).long()\n",
        "\n",
        "k = -1 # current index\n",
        "for c_y in unseen_cy:\n",
        "    k += 1\n",
        "    embed = c_y.repeat(1, 100).view(100, -1)  # 100 *312\n",
        "    lab = (unseen_y[k]).repeat(1, 100).view(100) # 100 labels\n",
        "    rand_noise = torch.randn(100, 312, device=device)  # generate 100 features\n",
        "    gen_inp = torch.cat((rand_noise, embed), dim=1)\n",
        "    generated = generator_model(gen_inp)  # 100 * 2048 : 100 features generated \n",
        "    x_train = torch.cat((x_train, generated), dim=0)\n",
        "    y_train = torch.cat((y_train, lab), dim=0)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "# print(torch.unique(y_train))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([12057, 2048])\n",
            "torch.Size([12057])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG-iG8Q4NqhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Final_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Final_Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Linear(2048, 200),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    # computes 200*200 confusion matrix for relevant classes\n",
        "    \n",
        "    def compute_confusion_matrix(self, inputs, classes):\n",
        "        per_class_acc = 0.0\n",
        "        nb_classes = 200\n",
        "        confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                    confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "        \n",
        "        return(confusion_matrix)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Fa9uE1wlyF",
        "colab_type": "code",
        "outputId": "96306595-cb83-4e2f-9f19-6828e15f19c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "softmax_cls = Final_Classifier()\n",
        "softmax_cls = softmax_cls.to(device)\n",
        "softmax_cls.weights_init()\n",
        "\n",
        "num_iters = 100\n",
        "lr = 0.0009\n",
        "cls_optimizer = optim.Adam(softmax_cls.parameters(), lr=lr, betas=(0.5,0.999))\n",
        "\n",
        "for ep in range(num_iters):\n",
        "    final_preds = softmax_cls(x_train)\n",
        "    loss = F.cross_entropy(final_preds, y_train)\n",
        "    cls_optimizer.zero_grad()\n",
        "    loss.backward(retain_graph=True)\n",
        "    cls_optimizer.step()\n",
        "    \n",
        "    print(\"loss after \", ep + 1, \"iters \", loss.item())\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss after  1 iters  5.298283100128174\n",
            "loss after  2 iters  5.29534387588501\n",
            "loss after  3 iters  5.285635471343994\n",
            "loss after  4 iters  5.26319694519043\n",
            "loss after  5 iters  5.2499165534973145\n",
            "loss after  6 iters  5.244321346282959\n",
            "loss after  7 iters  5.2376604080200195\n",
            "loss after  8 iters  5.231090068817139\n",
            "loss after  9 iters  5.224180698394775\n",
            "loss after  10 iters  5.218606948852539\n",
            "loss after  11 iters  5.208885669708252\n",
            "loss after  12 iters  5.208125591278076\n",
            "loss after  13 iters  5.201940536499023\n",
            "loss after  14 iters  5.197984218597412\n",
            "loss after  15 iters  5.200057029724121\n",
            "loss after  16 iters  5.193561553955078\n",
            "loss after  17 iters  5.19141960144043\n",
            "loss after  18 iters  5.190131187438965\n",
            "loss after  19 iters  5.184701442718506\n",
            "loss after  20 iters  5.180330276489258\n",
            "loss after  21 iters  5.175145626068115\n",
            "loss after  22 iters  5.171672344207764\n",
            "loss after  23 iters  5.1809821128845215\n",
            "loss after  24 iters  5.173696517944336\n",
            "loss after  25 iters  5.171682357788086\n",
            "loss after  26 iters  5.168436527252197\n",
            "loss after  27 iters  5.166337966918945\n",
            "loss after  28 iters  5.165921688079834\n",
            "loss after  29 iters  5.165072441101074\n",
            "loss after  30 iters  5.164237022399902\n",
            "loss after  31 iters  5.163008689880371\n",
            "loss after  32 iters  5.1604719161987305\n",
            "loss after  33 iters  5.160396099090576\n",
            "loss after  34 iters  5.160897731781006\n",
            "loss after  35 iters  5.158761024475098\n",
            "loss after  36 iters  5.158608436584473\n",
            "loss after  37 iters  5.1557464599609375\n",
            "loss after  38 iters  5.182331085205078\n",
            "loss after  39 iters  5.162802219390869\n",
            "loss after  40 iters  5.16403865814209\n",
            "loss after  41 iters  5.167226314544678\n",
            "loss after  42 iters  5.168075084686279\n",
            "loss after  43 iters  5.165649890899658\n",
            "loss after  44 iters  5.164109706878662\n",
            "loss after  45 iters  5.163650035858154\n",
            "loss after  46 iters  5.1622633934021\n",
            "loss after  47 iters  5.158966541290283\n",
            "loss after  48 iters  5.159149169921875\n",
            "loss after  49 iters  5.1573872566223145\n",
            "loss after  50 iters  5.1573686599731445\n",
            "loss after  51 iters  5.156534671783447\n",
            "loss after  52 iters  5.155879974365234\n",
            "loss after  53 iters  5.155773639678955\n",
            "loss after  54 iters  5.155699729919434\n",
            "loss after  55 iters  5.155332565307617\n",
            "loss after  56 iters  5.1551833152771\n",
            "loss after  57 iters  5.155056953430176\n",
            "loss after  58 iters  5.154971122741699\n",
            "loss after  59 iters  5.154824256896973\n",
            "loss after  60 iters  5.154721260070801\n",
            "loss after  61 iters  5.154616355895996\n",
            "loss after  62 iters  5.154520511627197\n",
            "loss after  63 iters  5.154428482055664\n",
            "loss after  64 iters  5.154327392578125\n",
            "loss after  65 iters  5.154243469238281\n",
            "loss after  66 iters  5.154195308685303\n",
            "loss after  67 iters  5.154118061065674\n",
            "loss after  68 iters  5.1540422439575195\n",
            "loss after  69 iters  5.153981685638428\n",
            "loss after  70 iters  5.153898239135742\n",
            "loss after  71 iters  5.153792858123779\n",
            "loss after  72 iters  5.1537604331970215\n",
            "loss after  73 iters  5.153727054595947\n",
            "loss after  74 iters  5.153690338134766\n",
            "loss after  75 iters  5.153936386108398\n",
            "loss after  76 iters  5.153558254241943\n",
            "loss after  77 iters  5.153428554534912\n",
            "loss after  78 iters  5.153385162353516\n",
            "loss after  79 iters  5.153343677520752\n",
            "loss after  80 iters  5.153276443481445\n",
            "loss after  81 iters  5.153236389160156\n",
            "loss after  82 iters  5.153229236602783\n",
            "loss after  83 iters  5.153264045715332\n",
            "loss after  84 iters  5.1531453132629395\n",
            "loss after  85 iters  5.15310525894165\n",
            "loss after  86 iters  5.153056621551514\n",
            "loss after  87 iters  5.153036594390869\n",
            "loss after  88 iters  5.1529541015625\n",
            "loss after  89 iters  5.153013229370117\n",
            "loss after  90 iters  5.152843475341797\n",
            "loss after  91 iters  5.152758598327637\n",
            "loss after  92 iters  5.152717113494873\n",
            "loss after  93 iters  5.154596328735352\n",
            "loss after  94 iters  5.15290641784668\n",
            "loss after  95 iters  5.152894496917725\n",
            "loss after  96 iters  5.152814865112305\n",
            "loss after  97 iters  5.152687072753906\n",
            "loss after  98 iters  5.152662754058838\n",
            "loss after  99 iters  5.152643203735352\n",
            "loss after  100 iters  5.152535438537598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohbHr0wVT4kM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f2096b22-854d-4ecb-ac43-d9cb7bb853c2"
      },
      "source": [
        "# Final Test and accuracy computation\n",
        "\n",
        "test_model = Final_Classifier()\n",
        "test_model = test_model.to(device)\n",
        "test_model.weights_init()\n",
        "\n",
        "# compute confusion matrix for seen and unseen classes separately\n",
        "\n",
        "cm_unseen = test_model.compute_confusion_matrix(x_train[7057:], y_train[7057:])\n",
        "cm_seen = test_model.compute_confusion_matrix(x_train[:7057], y_train[:7057])\n",
        "\n",
        "# compute per class accuracy matrix\n",
        "\n",
        "acc_mat_us = ((cm_unseen.diag()/cm_unseen.sum(1)))\n",
        "acc_mat_s = ((cm_seen.diag()/cm_seen.sum(1)))\n",
        "\n",
        "unseen_acc = []\n",
        "seen_acc = []\n",
        "\n",
        "# Remove Nan's from irrelevant classes\n",
        "\n",
        "for acc in acc_mat_us:\n",
        "    if not math.isnan(acc):\n",
        "        unseen_acc.append(acc)\n",
        "\n",
        "for acc in acc_mat_s:\n",
        "    if not math.isnan(acc):\n",
        "        seen_acc.append(acc)\n",
        "\n",
        "# Compute per class accuracy\n",
        "\n",
        "per_class_seen = np.mean(seen_acc)\n",
        "per_class_unseen = np.mean(unseen_acc)\n",
        "l =[per_class_seen, per_class_unseen]\n",
        "\n",
        "print(\"unseen classs accuray for zsl is :\", per_class_unseen)\n",
        "print(\"seen classs accuray is :\", per_class_seen)\n",
        "print(\"harmonic mean for generalized zsl is :\", statistics.harmonic_mean(l))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unseen classs accuray for zsl is : 0.0026\n",
            "seen classs accuray is : 0.0024143413\n",
            "harmonic mean for generalized zsl is : 0.002503733605770536\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}