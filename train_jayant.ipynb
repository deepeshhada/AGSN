{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZSL - SABR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepeshhada/SABR/blob/master/train_jayant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCGCNUyKVqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVkj8xk-jbmo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25365875-86f1-4bb3-f8b6-b2b82ba5bf4c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc_Hx75kNwVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Sz3S_oTgW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set dataset from: CUB, SUN, AWA, AWA2, APY\n",
        "_dataset = \"CUB\"\n",
        "data_root = \"./drive/My Drive/ZSL Datasets/\" + _dataset + \"/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPj6eYTRrd3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels, class_embeddings):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.class_embeddings = class_embeddings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        return (self.features[index], label, class_embeddings[label])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmE2ATJX6MT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7712c0a8-19de-45b8-f4ff-42881891f45e"
      },
      "source": [
        "# load mat files\n",
        "res101 = io.loadmat(data_root + \"res101.mat\")\n",
        "att_splits = io.loadmat(data_root + \"att_splits.mat\")\n",
        "\n",
        "resnet_features = res101['features'].T\n",
        "class_labels = res101['labels']\n",
        "class_embeddings = att_splits['att'].T\n",
        "\n",
        "print(att_splits['trainval_loc'])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[10877]\n",
            " [10921]\n",
            " [ 2653]\n",
            " ...\n",
            " [ 5227]\n",
            " [11508]\n",
            " [ 1643]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB8Zw4NJUtt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_splits(loc, shuffle=False):\n",
        "    indices = att_splits[loc].reshape(-1) - 1\n",
        "    # print(len(indices))\n",
        "    features = resnet_features[indices]\n",
        "    labels = class_labels[indices].reshape(-1) - 1\n",
        "\n",
        "    split = Dataset(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        class_embeddings=class_embeddings\n",
        "    )\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=split,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return split, dataloader\n",
        "\n",
        "\n",
        "train_set, trainloader = generate_splits(loc='trainval_loc', shuffle=True)\n",
        "seen_test_set, seen_testloader = generate_splits(loc='test_seen_loc', shuffle=False)\n",
        "unseen_test_set, unseen_testloader = generate_splits(loc='test_unseen_loc', shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drasv2E2fN7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the transformation, Î¨, which operates on 2048-dimensional resnet features.\n",
        "# May have to remove this.\n",
        "\n",
        "# class LatentTransform(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(LatentTransform, self).__init__()\n",
        "#         self.model = nn.Sequential(\n",
        "#             nn.Linear(in_features=2048, out_features=2048, bias=True),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(in_features=2048, out_features=1024, bias=True),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         return self.model(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fena8z4V-Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_initialize(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(0.0, 0.02)\n",
        "        module.bias.data.normal_(0.0, 0.02)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is0I5qID6Uzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this in sync with the Generator\n",
        "# Generator class looks similar to the \"LatentTransform\" class\n",
        "# the out_features of both the classifier and regressor are hardcoded for now.\n",
        "# TODO: make the out_features generic.\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=150, bias=True), # Earlier out_features set to 200 but changed to 150\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        return F.cross_entropy(pred, true)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.model = nn.Linear(in_features=2048, out_features=312, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def loss(self, true, pred, b_size):   # to be checked for possible errors..\n",
        "        # true, pred --> batch_size * 312\n",
        "        a = F.normalize(true, p=2, dim=1, eps=1e-8).to(device)\n",
        "        b = F.normalize(pred, p=2, dim=1, eps=1e-8).to(device)\n",
        "        similarity_scores = torch.mm(a, b.T)  # batch * batch\n",
        "        arr = np.arange(0,b_size)\n",
        "        target = torch.tensor(arr).to(device).long()\n",
        "        loss = F.cross_entropy(similarity_scores, target)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):  # removed c_y from signature\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "        # norm = torch.norm(input=x, p=2, dim=1).detach()\n",
        "        # x = x.div(norm.expand_as(x))\n",
        "        # return torch.bmm(x, c_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Pb5HxI80U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=624, out_features=2048, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=2048, out_features=2048),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBYnxnOEDN1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2360, out_features=4096, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=1),\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yssq9qh_M3VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_model = Generator()\n",
        "discriminator_model = Discriminator()\n",
        "classifier_model = Classifier()\n",
        "regressor_model = Regressor()\n",
        "\n",
        "generator_model = generator_model.to(device)\n",
        "discriminator_model = discriminator_model.to(device)\n",
        "classifier_model = classifier_model.to(device)\n",
        "regressor_model = regressor_model.to(device)\n",
        "\n",
        "generator_model.weights_init()\n",
        "discriminator_model.weights_init()\n",
        "classifier_model.weights_init()\n",
        "regressor_model.weights_init()\n",
        "\n",
        "learning_rate = 0.001\n",
        "gamma = 0.01\n",
        "beeta = 0.1\n",
        "lamda = 10\n",
        "num_epochs = 20\n",
        "\n",
        "train_params = list(generator_model.parameters()) + list(classifier_model.parameters()) + list(regressor_model.parameters())\n",
        "G_optimizer = optim.Adam(train_params, lr=learning_rate, betas=(0.5,0.999))\n",
        "D_optimizer = optim.Adam(discriminator_model.parameters(), lr = learning_rate, betas=(0.5,0.999))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GYE6jAdXe89",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1c349d8-30d8-479b-b20c-eb4447dcf689"
      },
      "source": [
        "for ep in range(20):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        f, l, e = data\n",
        "        features, labels, embeddings = f.to(device).float(), l.to(device).long(), e.to(device).float()\n",
        "        discriminator_model.zero_grad()\n",
        "        b_size = embeddings.shape[0]\n",
        "        noise = torch.randn(b_size, 312, device=device)\n",
        "        gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "        fake_features = generator_model(gen_input)\n",
        "        align_cls = classifier_model(fake_features)\n",
        "        cls_loss = classifier_model.loss(labels, align_cls)  # Computing classifier loss\n",
        "        # print(cls_loss)\n",
        "        align_reg = regressor_model(fake_features)\n",
        "        reg_loss = regressor_model.loss(embeddings, align_reg, b_size) # Computing Regressor loss\n",
        "        # print(reg_loss)\n",
        "\n",
        "        # Discriminator Loss\n",
        "\n",
        "        for k in range(2):\n",
        "            # alpha = torch.rand((batch_size, 1)).to(device) # Random [0,1) from uniform dist.\n",
        "            # interpolate = alpha*features + (1 - alpha)*fake_features   # b_size * 2048\n",
        "\n",
        "            # train with real features\n",
        "            disc_input_real = torch.cat((features, embeddings), dim=1)\n",
        "            disc_score_real = discriminator_model(disc_input_real)\n",
        "            disc_loss_real = torch.mean(disc_score_real)\n",
        "\n",
        "            # train with fake features\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = generator_model(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = discriminator_model(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            \n",
        "            disc_loss_total = torch.mean(disc_score_fake - disc_score_real)  # Still have to implement grad penalty\n",
        "            # print(disc_loss_total)\n",
        "            disc_loss_total.backward(retain_graph=True)\n",
        "            D_optimizer.step()\n",
        "        \n",
        "\n",
        "        # Wasserstein Generator loss\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            generator_model.zero_grad()\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = generator_model(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = discriminator_model(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            gen_loss1 = -disc_loss_fake\n",
        "            gen_loss = gen_loss1 + beeta *(cls_loss + (gamma * reg_loss))\n",
        "            # print(gen_loss)\n",
        "            gen_loss.backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "    print(\"discriminator loss after\", ep+1, \" iterations\", disc_loss_total.item())\n",
        "    print(\"generator loss after\", ep+1, \" iterations\", gen_loss.item())\n",
        "    print()\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "discriminator loss after 1  iterations -11185.9423828125\n",
            "generator loss after 1  iterations 13558.2099609375\n",
            "\n",
            "discriminator loss after 2  iterations -3585.72021484375\n",
            "generator loss after 2  iterations 4116.03857421875\n",
            "\n",
            "discriminator loss after 3  iterations -1798.328857421875\n",
            "generator loss after 3  iterations 4851.34521484375\n",
            "\n",
            "discriminator loss after 4  iterations -826.8648071289062\n",
            "generator loss after 4  iterations -43.62324905395508\n",
            "\n",
            "discriminator loss after 5  iterations -479.456298828125\n",
            "generator loss after 5  iterations 2981.839111328125\n",
            "\n",
            "discriminator loss after 6  iterations -538.539306640625\n",
            "generator loss after 6  iterations 2051.4794921875\n",
            "\n",
            "discriminator loss after 7  iterations -255.76568603515625\n",
            "generator loss after 7  iterations 1659.943359375\n",
            "\n",
            "discriminator loss after 8  iterations -425.21673583984375\n",
            "generator loss after 8  iterations 2102.593505859375\n",
            "\n",
            "discriminator loss after 9  iterations 228.62648010253906\n",
            "generator loss after 9  iterations 1086.9281005859375\n",
            "\n",
            "discriminator loss after 10  iterations -685.7152099609375\n",
            "generator loss after 10  iterations 2175.686279296875\n",
            "\n",
            "discriminator loss after 11  iterations -337.41656494140625\n",
            "generator loss after 11  iterations 1385.3255615234375\n",
            "\n",
            "discriminator loss after 12  iterations -327.2829284667969\n",
            "generator loss after 12  iterations 1136.185791015625\n",
            "\n",
            "discriminator loss after 13  iterations -142.37033081054688\n",
            "generator loss after 13  iterations 836.8815307617188\n",
            "\n",
            "discriminator loss after 14  iterations 217.67312622070312\n",
            "generator loss after 14  iterations 4698.5673828125\n",
            "\n",
            "discriminator loss after 15  iterations 100.76983642578125\n",
            "generator loss after 15  iterations 2259.318115234375\n",
            "\n",
            "discriminator loss after 16  iterations -749.59033203125\n",
            "generator loss after 16  iterations 2192.851806640625\n",
            "\n",
            "discriminator loss after 17  iterations -697.7472534179688\n",
            "generator loss after 17  iterations 1113.219970703125\n",
            "\n",
            "discriminator loss after 18  iterations 1640.337158203125\n",
            "generator loss after 18  iterations 3721.887939453125\n",
            "\n",
            "discriminator loss after 19  iterations 583.8704223632812\n",
            "generator loss after 19  iterations 2741.376220703125\n",
            "\n",
            "discriminator loss after 20  iterations 244.72190856933594\n",
            "generator loss after 20  iterations 5535.65185546875\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}