{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZSL - SABR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepeshhada/SABR/blob/master/train_jayant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCGCNUyKVqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVkj8xk-jbmo",
        "colab_type": "code",
        "outputId": "775af64f-c383-4b58-e708-38954a6b6d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc_Hx75kNwVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Sz3S_oTgW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set dataset from: CUB, SUN, AWA, AWA2, APY\n",
        "_dataset = \"CUB\"\n",
        "data_root = \"./drive/My Drive/ZSL Datasets/\" + _dataset + \"/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPj6eYTRrd3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels, class_embeddings):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.class_embeddings = class_embeddings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        return (self.features[index], label, class_embeddings[label])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmE2ATJX6MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load mat files\n",
        "res101 = io.loadmat(data_root + \"res101.mat\")\n",
        "att_splits = io.loadmat(data_root + \"att_splits.mat\")\n",
        "\n",
        "resnet_features = res101['features'].T\n",
        "class_labels = res101['labels']\n",
        "class_embeddings = att_splits['att'].T\n",
        "\n",
        "# print((att_splits['trainval_loc']).reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB8Zw4NJUtt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "926c1b12-1cf9-4825-c564-1a98e7963495"
      },
      "source": [
        "def generate_splits(loc, shuffle=False):\n",
        "    indices = att_splits[loc].reshape(-1) - 1\n",
        "    features = resnet_features[indices]\n",
        "    labels = class_labels[indices].reshape(-1) - 1\n",
        "\n",
        "    split = Dataset(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        class_embeddings=class_embeddings\n",
        "    )\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=split,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return split, dataloader\n",
        "\n",
        "\n",
        "train_set, trainloader = generate_splits(loc='trainval_loc', shuffle=True)\n",
        "seen_test_set, seen_testloader = generate_splits(loc='test_seen_loc', shuffle=False)\n",
        "unseen_test_set, unseen_testloader = generate_splits(loc='test_unseen_loc', shuffle=False)\n",
        "print(train_set.labels[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaAtDuK4i4k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unseen_labels = np.unique(unseen_test_set.labels)\n",
        "seen_labels = np.unique(train_set.labels)\n",
        "\n",
        "unseen = class_embeddings[unseen_labels]\n",
        "seen = class_embeddings[seen_labels]\n",
        "\n",
        "unseen_cy = torch.tensor(unseen, device=device).float()\n",
        "seen_cy = torch.tensor(seen, device=device).float()\n",
        "unseen_y = torch.tensor(unseen_labels, device=device).long()\n",
        "seen_y = torch.tensor(seen_labels, device=device).long()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fena8z4V-Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_initialize(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(0.0, 0.02)\n",
        "        module.bias.data.normal_(0.0, 0.02)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is0I5qID6Uzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this in sync with the Generator\n",
        "# Generator class looks similar to the \"LatentTransform\" class\n",
        "# the out_features of both the classifier and regressor are hardcoded for now.\n",
        "# TODO: make the out_features generic.\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=150, bias=True), # Earlier out_features set to 200 but changed to 150\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        return F.cross_entropy(pred, true)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.model = nn.Linear(in_features=2048, out_features=312, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    # def loss(self, true, pred, b_size):   # to be checked for possible errors..\n",
        "    #     # true, pred --> batch_size * 312\n",
        "    #     a = F.normalize(true, p=2, dim=1, eps=1e-8).to(device)\n",
        "    #     b = F.normalize(pred, p=2, dim=1, eps=1e-8).to(device)\n",
        "    #     similarity_scores = torch.mm(a, b.T)  # batch * batch\n",
        "    #     arr = np.arange(0,b_size)\n",
        "    #     target = torch.tensor(arr).to(device).long()\n",
        "    #     loss = F.cross_entropy(similarity_scores, target)\n",
        "    #     return loss\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        similarity_scores = torch.mm(pred, seen_cy.T) # batch * 150\n",
        "        loss = F.cross_entropy(similarity_scores, true)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):  # removed c_y from signature\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "        # norm = torch.norm(input=x, p=2, dim=1).detach()\n",
        "        # x = x.div(norm.expand_as(x))\n",
        "        # return torch.bmm(x, c_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Pb5HxI80U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=624, out_features=2048, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=2048, out_features=2048),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBYnxnOEDN1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2360, out_features=4096, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=1),\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yssq9qh_M3VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_model = Generator()\n",
        "discriminator_model = Discriminator()\n",
        "classifier_model = Classifier()\n",
        "regressor_model = Regressor()\n",
        "\n",
        "generator_model = generator_model.to(device)\n",
        "discriminator_model = discriminator_model.to(device)\n",
        "classifier_model = classifier_model.to(device)\n",
        "regressor_model = regressor_model.to(device)\n",
        "\n",
        "generator_model.weights_init()\n",
        "discriminator_model.weights_init()\n",
        "classifier_model.weights_init()\n",
        "regressor_model.weights_init()\n",
        "\n",
        "learning_rate = 0.001\n",
        "gamma = 0.01\n",
        "beeta = 0.1\n",
        "lamda = 10\n",
        "num_epochs = 20\n",
        "\n",
        "train_params = list(generator_model.parameters()) + list(classifier_model.parameters()) + list(regressor_model.parameters())\n",
        "G_optimizer = optim.Adam(train_params, lr=learning_rate, betas=(0.5,0.999))\n",
        "D_optimizer = optim.Adam(discriminator_model.parameters(), lr = learning_rate, betas=(0.5,0.999))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GYE6jAdXe89",
        "colab_type": "code",
        "outputId": "43826738-3c69-4cfa-b188-75bed03b7560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "for ep in range(10):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        f, l, e = data\n",
        "        features, labels, embeddings = f.to(device).float(), l.to(device).long(), e.to(device).float()\n",
        "        discriminator_model.zero_grad()\n",
        "        b_size = embeddings.shape[0]\n",
        "        noise = torch.randn(b_size, 312, device=device)\n",
        "        gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "        fake_features = generator_model(gen_input)\n",
        "        align_cls = classifier_model(fake_features)\n",
        "        cls_loss = classifier_model.loss(labels, align_cls)  # Computing classifier loss\n",
        "        # print(cls_loss)\n",
        "        align_reg = regressor_model(fake_features)\n",
        "        reg_loss = regressor_model.loss(labels, align_reg) # Computing Regressor loss\n",
        "        # print(reg_loss)\n",
        "\n",
        "        # Discriminator Loss\n",
        "\n",
        "        for k in range(2):\n",
        "            # alpha = torch.rand((batch_size, 1)).to(device) # Random [0,1) from uniform dist.\n",
        "            # interpolate = alpha*features + (1 - alpha)*fake_features   # b_size * 2048\n",
        "\n",
        "            # train with real features\n",
        "            disc_input_real = torch.cat((features, embeddings), dim=1)\n",
        "            disc_score_real = discriminator_model(disc_input_real)\n",
        "            disc_loss_real = torch.mean(disc_score_real)\n",
        "\n",
        "            # train with fake features\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = generator_model(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = discriminator_model(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            \n",
        "            disc_loss_total = torch.mean(disc_score_fake - disc_score_real)  # Still have to implement grad penalty\n",
        "            # print(disc_loss_total)\n",
        "            disc_loss_total.backward(retain_graph=True)\n",
        "            D_optimizer.step()\n",
        "        \n",
        "\n",
        "        # Wasserstein Generator loss\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            generator_model.zero_grad()\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = generator_model(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = discriminator_model(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            gen_loss1 = -disc_loss_fake\n",
        "            gen_loss = gen_loss1 + beeta *(cls_loss + (gamma * reg_loss))\n",
        "            # print(gen_loss)\n",
        "            gen_loss.backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "    print(\"discriminator loss after\", ep+1, \" iterations\", disc_loss_total.item())\n",
        "    print(\"generator loss after\", ep+1, \" iterations\", gen_loss.item())\n",
        "    print()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "discriminator loss after 1  iterations -4236.0498046875\n",
            "generator loss after 1  iterations -711.643310546875\n",
            "\n",
            "discriminator loss after 2  iterations -3390.333251953125\n",
            "generator loss after 2  iterations 4916.939453125\n",
            "\n",
            "discriminator loss after 3  iterations -1322.494873046875\n",
            "generator loss after 3  iterations 1124.317138671875\n",
            "\n",
            "discriminator loss after 4  iterations 725.8442993164062\n",
            "generator loss after 4  iterations -62.28472900390625\n",
            "\n",
            "discriminator loss after 5  iterations -1125.94921875\n",
            "generator loss after 5  iterations 632.861328125\n",
            "\n",
            "discriminator loss after 6  iterations -76.3913803100586\n",
            "generator loss after 6  iterations 1744.808837890625\n",
            "\n",
            "discriminator loss after 7  iterations -334.569580078125\n",
            "generator loss after 7  iterations 516.9773559570312\n",
            "\n",
            "discriminator loss after 8  iterations -996.8807373046875\n",
            "generator loss after 8  iterations 3550.105224609375\n",
            "\n",
            "discriminator loss after 9  iterations -1357.279541015625\n",
            "generator loss after 9  iterations 1484.5313720703125\n",
            "\n",
            "discriminator loss after 10  iterations -1072.3612060546875\n",
            "generator loss after 10  iterations 2619.2705078125\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqnsC2-RPd2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "028063e9-3299-4cfd-cac5-311d9b762376"
      },
      "source": [
        "# generating features from the unseen classes using trained generator\n",
        "\n",
        "x_train = torch.tensor(train_set.features, device=device).float()\n",
        "y_train = torch.tensor(train_set.labels, device=device).long()\n",
        "\n",
        "k = -1 # current index\n",
        "for c_y in unseen_cy:\n",
        "    k += 1\n",
        "    embed = c_y.repeat(1, 100).view(100, -1)  # 100 *312\n",
        "    lab = (unseen_y[k]).repeat(1, 100).view(100) # 100 labels\n",
        "    rand_noise = torch.randn(100, 312, device=device)  # generate 100 features\n",
        "    gen_inp = torch.cat((rand_noise, embed), dim=1)\n",
        "    generated = generator_model(gen_inp)  # 100 * 2048 : 100 features generated \n",
        "    x_train = torch.cat((x_train, generated), dim=0)\n",
        "    y_train = torch.cat((y_train, lab), dim=0)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "# print(torch.unique(y_train))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([12057, 2048])\n",
            "torch.Size([12057])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG-iG8Q4NqhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Final_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Final_Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Linear(2048, 200),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    # to be written\n",
        "    def compute_per_class_accuracy_gzsl(self, true_labels, pred_labels):\n",
        "        per_class_acc = 0.0\n",
        "        return per_class_acc\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Fa9uE1wlyF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "ac993343-c5d9-4a7a-9bd3-5a75985f460d"
      },
      "source": [
        "softmax_cls = Final_Classifier()\n",
        "softmax_cls = softmax_cls.to(device)\n",
        "softmax_cls.weights_init()\n",
        "\n",
        "num_iters = 50\n",
        "lr = 0.001\n",
        "cls_optimizer = optim.Adam(softmax_cls.parameters(), lr=lr, betas=(0.5,0.999))\n",
        "\n",
        "for ep in range(num_iters):\n",
        "    final_preds = softmax_cls(x_train)\n",
        "    loss = F.cross_entropy(final_preds, y_train)\n",
        "    cls_optimizer.zero_grad()\n",
        "    loss.backward(retain_graph=True)\n",
        "    cls_optimizer.step()\n",
        "    \n",
        "    print(\"loss after \", ep + 1, \"iters \", loss.item())\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss after  1 iters  5.298337459564209\n",
            "loss after  2 iters  5.295351028442383\n",
            "loss after  3 iters  5.285797595977783\n",
            "loss after  4 iters  5.2744832038879395\n",
            "loss after  5 iters  5.263491153717041\n",
            "loss after  6 iters  5.260190963745117\n",
            "loss after  7 iters  5.246638774871826\n",
            "loss after  8 iters  5.228035926818848\n",
            "loss after  9 iters  5.226659297943115\n",
            "loss after  10 iters  5.2174072265625\n",
            "loss after  11 iters  5.2051920890808105\n",
            "loss after  12 iters  5.203485012054443\n",
            "loss after  13 iters  5.199697494506836\n",
            "loss after  14 iters  5.189092636108398\n",
            "loss after  15 iters  5.185328483581543\n",
            "loss after  16 iters  5.172701358795166\n",
            "loss after  17 iters  5.1678972244262695\n",
            "loss after  18 iters  5.161052227020264\n",
            "loss after  19 iters  5.160949230194092\n",
            "loss after  20 iters  5.159963130950928\n",
            "loss after  21 iters  5.155332565307617\n",
            "loss after  22 iters  5.147940635681152\n",
            "loss after  23 iters  5.141739845275879\n",
            "loss after  24 iters  5.148510932922363\n",
            "loss after  25 iters  5.1422624588012695\n",
            "loss after  26 iters  5.142632007598877\n",
            "loss after  27 iters  5.134673118591309\n",
            "loss after  28 iters  5.136116027832031\n",
            "loss after  29 iters  5.142869472503662\n",
            "loss after  30 iters  5.1330342292785645\n",
            "loss after  31 iters  5.1321539878845215\n",
            "loss after  32 iters  5.126420497894287\n",
            "loss after  33 iters  5.129603862762451\n",
            "loss after  34 iters  5.128086090087891\n",
            "loss after  35 iters  5.12588357925415\n",
            "loss after  36 iters  5.126276016235352\n",
            "loss after  37 iters  5.123692512512207\n",
            "loss after  38 iters  5.122352123260498\n",
            "loss after  39 iters  5.121050834655762\n",
            "loss after  40 iters  5.121775150299072\n",
            "loss after  41 iters  5.120652198791504\n",
            "loss after  42 iters  5.120244026184082\n",
            "loss after  43 iters  5.1198225021362305\n",
            "loss after  44 iters  5.119422435760498\n",
            "loss after  45 iters  5.122682571411133\n",
            "loss after  46 iters  5.120924949645996\n",
            "loss after  47 iters  5.119220733642578\n",
            "loss after  48 iters  5.117406845092773\n",
            "loss after  49 iters  5.115911483764648\n",
            "loss after  50 iters  5.1157426834106445\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}