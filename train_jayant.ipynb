{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZSL - SABR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepeshhada/SABR/blob/master/train_jayant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCCGCNUyKVqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import math\n",
        "import statistics \n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVkj8xk-jbmo",
        "colab_type": "code",
        "outputId": "2d9eb391-eb25-4616-de5f-1700592c8fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc_Hx75kNwVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Sz3S_oTgW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set dataset from: CUB, SUN, AWA, AWA2, APY\n",
        "_dataset = \"CUB\"\n",
        "data_root = \"./drive/My Drive/ZSL Datasets/\" + _dataset + \"/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPj6eYTRrd3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels, class_embeddings):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.class_embeddings = class_embeddings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.labels[index]\n",
        "        return (self.features[index], label, class_embeddings[label])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LmE2ATJX6MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load mat files\n",
        "res101 = io.loadmat(data_root + \"res101.mat\")\n",
        "att_splits = io.loadmat(data_root + \"att_splits.mat\")\n",
        "\n",
        "resnet_features = res101['features'].T\n",
        "class_labels = res101['labels']\n",
        "class_embeddings = att_splits['att'].T\n",
        "\n",
        "# print((att_splits['trainval_loc']).reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB8Zw4NJUtt6",
        "colab_type": "code",
        "outputId": "8a792479-0fb6-44da-c60a-77201d0d1df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def generate_splits(loc, shuffle=False):\n",
        "    indices = att_splits[loc].reshape(-1) - 1\n",
        "    features = resnet_features[indices]\n",
        "    labels = class_labels[indices].reshape(-1) - 1\n",
        "\n",
        "    split = Dataset(\n",
        "        features=features,\n",
        "        labels=labels,\n",
        "        class_embeddings=class_embeddings\n",
        "    )\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset=split,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return split, dataloader\n",
        "\n",
        "\n",
        "train_set, trainloader = generate_splits(loc='trainval_loc', shuffle=True)\n",
        "seen_test_set, seen_testloader = generate_splits(loc='test_seen_loc', shuffle=False)\n",
        "unseen_test_set, unseen_testloader = generate_splits(loc='test_unseen_loc', shuffle=False)\n",
        "print(train_set.labels[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaAtDuK4i4k0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "35c6dca3-ca90-432e-ebc6-a53cdccd772d"
      },
      "source": [
        "unseen_labels = np.unique(unseen_test_set.labels)\n",
        "seen_labels = np.unique(train_set.labels)\n",
        "\n",
        "unseen = class_embeddings[unseen_labels]\n",
        "seen = class_embeddings[seen_labels]\n",
        "\n",
        "unseen_cy = torch.tensor(unseen, device=device).float()\n",
        "seen_cy = torch.tensor(seen, device=device).float()\n",
        "unseen_y = torch.tensor(unseen_labels, device=device).long()\n",
        "seen_y = torch.tensor(seen_labels, device=device).long()\n",
        "print(unseen_y)\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  6,  18,  20,  28,  33,  35,  49,  55,  61,  67,  68,  71,  78,  79,\n",
            "         86,  87,  90,  94,  97,  99, 103, 107, 115, 119, 121, 123, 124, 128,\n",
            "        138, 140, 141, 149, 151, 156, 158, 159, 165, 166, 170, 173, 175, 178,\n",
            "        181, 184, 186, 188, 190, 191, 192, 194], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fena8z4V-Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normal_initialize(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        module.weight.data.normal_(0.0, 0.02)\n",
        "        module.bias.data.normal_(0.0, 0.02)\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is0I5qID6Uzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this in sync with the Generator\n",
        "# Generator class looks similar to the \"LatentTransform\" class\n",
        "# the out_features of both the classifier and regressor are hardcoded for now.\n",
        "# TODO: make the out_features generic.\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2048, out_features=150, bias=True), # Earlier out_features set to 200 but changed to 150\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        return F.cross_entropy(pred, true)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.model = nn.Linear(in_features=2048, out_features=312, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "\n",
        "    def loss(self, true, pred):\n",
        "        similarity_scores = torch.mm(pred, seen_cy.T) # batch * 150\n",
        "        loss = F.cross_entropy(similarity_scores, true)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):  # removed c_y from signature\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "        # norm = torch.norm(input=x, p=2, dim=1).detach()\n",
        "        # x = x.div(norm.expand_as(x))\n",
        "        # return torch.bmm(x, c_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Pb5HxI80U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=624, out_features=2048, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=2048, out_features=2048),\n",
        "            nn.ReLU(inplace=False)\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBYnxnOEDN1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_features=2360, out_features=4096, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.01, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=1),\n",
        "        )\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yssq9qh_M3VG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_model = Generator()\n",
        "discriminator_model = Discriminator()\n",
        "classifier_model = Classifier()\n",
        "regressor_model = Regressor()\n",
        "\n",
        "generator_model = generator_model.to(device)\n",
        "discriminator_model = discriminator_model.to(device)\n",
        "classifier_model = classifier_model.to(device)\n",
        "regressor_model = regressor_model.to(device)\n",
        "\n",
        "generator_model.weights_init()\n",
        "discriminator_model.weights_init()\n",
        "classifier_model.weights_init()\n",
        "regressor_model.weights_init()\n",
        "\n",
        "learning_rate = 0.001\n",
        "gamma = 0.01\n",
        "beeta = 0.1\n",
        "lamda = 10\n",
        "num_epochs = 20\n",
        "\n",
        "train_params = list(generator_model.parameters()) + list(classifier_model.parameters()) + list(regressor_model.parameters())\n",
        "G_optimizer = optim.Adam(train_params, lr=learning_rate, betas=(0.5,0.999))\n",
        "D_optimizer = optim.Adam(discriminator_model.parameters(), lr = learning_rate, betas=(0.5,0.999))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GYE6jAdXe89",
        "colab_type": "code",
        "outputId": "6894b639-38a9-4036-f3f7-ffc7a6819ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "for ep in range(10):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        f, l, e = data\n",
        "        features, labels, embeddings = f.to(device).float(), l.to(device).long(), e.to(device).float()\n",
        "        discriminator_model.zero_grad()\n",
        "        b_size = embeddings.shape[0]\n",
        "        noise = torch.randn(b_size, 312, device=device)\n",
        "        gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "        fake_features = generator_model(gen_input)\n",
        "        align_cls = classifier_model(fake_features)\n",
        "        cls_loss = classifier_model.loss(labels, align_cls)  # Computing classifier loss\n",
        "        # print(cls_loss)\n",
        "        align_reg = regressor_model(fake_features)\n",
        "        reg_loss = regressor_model.loss(labels, align_reg) # Computing Regressor loss\n",
        "        # print(reg_loss)\n",
        "\n",
        "        # Discriminator Loss\n",
        "\n",
        "        for k in range(2):\n",
        "            # alpha = torch.rand((batch_size, 1)).to(device) # Random [0,1) from uniform dist.\n",
        "            # interpolate = alpha*features + (1 - alpha)*fake_features   # b_size * 2048\n",
        "\n",
        "            # train with real features\n",
        "            disc_input_real = torch.cat((features, embeddings), dim=1)\n",
        "            disc_score_real = discriminator_model(disc_input_real)\n",
        "            disc_loss_real = torch.mean(disc_score_real)\n",
        "\n",
        "            # train with fake features\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = generator_model(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = discriminator_model(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            \n",
        "            disc_loss_total = torch.mean(disc_score_fake - disc_score_real)  # Still have to implement grad penalty\n",
        "            # print(disc_loss_total)\n",
        "            disc_loss_total.backward(retain_graph=True)\n",
        "            D_optimizer.step()\n",
        "        \n",
        "\n",
        "        # Wasserstein Generator loss\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "            generator_model.zero_grad()\n",
        "            noise = torch.randn(b_size, 312, device=device)\n",
        "            gen_input = torch.cat((noise, embeddings), dim=1)\n",
        "            fake_features = generator_model(gen_input)\n",
        "            disc_input_fake = torch.cat((fake_features, embeddings), dim=1)\n",
        "            disc_score_fake = discriminator_model(disc_input_fake)\n",
        "            disc_loss_fake = torch.mean(disc_score_fake)\n",
        "            gen_loss1 = -disc_loss_fake\n",
        "            gen_loss = gen_loss1 + beeta *(cls_loss + (gamma * reg_loss))\n",
        "            # print(gen_loss)\n",
        "            gen_loss.backward()\n",
        "            G_optimizer.step()\n",
        "\n",
        "    print(\"discriminator loss after\", ep+1, \" iterations\", disc_loss_total.item())\n",
        "    print(\"generator loss after\", ep+1, \" iterations\", gen_loss.item())\n",
        "    print()\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "discriminator loss after 1  iterations -2225.86328125\n",
            "generator loss after 1  iterations 3975.901123046875\n",
            "\n",
            "discriminator loss after 2  iterations -1754.54736328125\n",
            "generator loss after 2  iterations 666.82958984375\n",
            "\n",
            "discriminator loss after 3  iterations -92.95471954345703\n",
            "generator loss after 3  iterations 1077.2305908203125\n",
            "\n",
            "discriminator loss after 4  iterations -915.5635986328125\n",
            "generator loss after 4  iterations 3851.5654296875\n",
            "\n",
            "discriminator loss after 5  iterations 0.1716093122959137\n",
            "generator loss after 5  iterations -359.4494323730469\n",
            "\n",
            "discriminator loss after 6  iterations -561.6085815429688\n",
            "generator loss after 6  iterations 780.5733642578125\n",
            "\n",
            "discriminator loss after 7  iterations -158.9719696044922\n",
            "generator loss after 7  iterations 776.2692260742188\n",
            "\n",
            "discriminator loss after 8  iterations 661.1727905273438\n",
            "generator loss after 8  iterations 2772.822021484375\n",
            "\n",
            "discriminator loss after 9  iterations 384.4087829589844\n",
            "generator loss after 9  iterations 5832.2470703125\n",
            "\n",
            "discriminator loss after 10  iterations -2277.613037109375\n",
            "generator loss after 10  iterations 5481.69189453125\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqnsC2-RPd2N",
        "colab_type": "code",
        "outputId": "5393b6b8-daaf-4ac3-a6fe-9f2cbff8a9ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# generating features from the unseen classes using trained generator\n",
        "\n",
        "x_train = torch.tensor(train_set.features, device=device).float()\n",
        "y_train = torch.tensor(train_set.labels, device=device).long()\n",
        "\n",
        "k = -1 # current index\n",
        "for c_y in unseen_cy:\n",
        "    k += 1\n",
        "    embed = c_y.repeat(1, 100).view(100, -1)  # 100 *312\n",
        "    lab = (unseen_y[k]).repeat(1, 100).view(100) # 100 labels\n",
        "    rand_noise = torch.randn(100, 312, device=device)  # generate 100 features\n",
        "    gen_inp = torch.cat((rand_noise, embed), dim=1)\n",
        "    generated = generator_model(gen_inp)  # 100 * 2048 : 100 features generated \n",
        "    x_train = torch.cat((x_train, generated), dim=0)\n",
        "    y_train = torch.cat((y_train, lab), dim=0)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "# print(torch.unique(y_train))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([12057, 2048])\n",
            "torch.Size([12057])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG-iG8Q4NqhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Final_Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Final_Classifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.LeakyReLU(negative_slope=0.01),\n",
        "            nn.Linear(2048, 200),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    def weights_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_initialize(self._modules[m])\n",
        "\n",
        "    # computes 200*200 confusion matrix for relevant classes\n",
        "    \n",
        "    def compute_confusion_matrix(self, inputs, classes):\n",
        "        per_class_acc = 0.0\n",
        "        nb_classes = 200\n",
        "        confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                    confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "        \n",
        "        return(confusion_matrix)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74Fa9uE1wlyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "softmax_cls = Final_Classifier()\n",
        "softmax_cls = softmax_cls.to(device)\n",
        "softmax_cls.weights_init()\n",
        "\n",
        "num_iters = 100\n",
        "lr = 0.0009\n",
        "cls_optimizer = optim.Adam(softmax_cls.parameters(), lr=lr, betas=(0.5,0.999))\n",
        "\n",
        "for ep in range(num_iters):\n",
        "    final_preds = softmax_cls(x_train)\n",
        "    loss = F.cross_entropy(final_preds, y_train)\n",
        "    cls_optimizer.zero_grad()\n",
        "    loss.backward(retain_graph=True)\n",
        "    cls_optimizer.step()\n",
        "    \n",
        "    print(\"loss after \", ep + 1, \"iters \", loss.item())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohbHr0wVT4kM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a8164d73-33d6-4a49-ffaa-75d6b94537ed"
      },
      "source": [
        "# Final Test and accuracy computation\n",
        "\n",
        "test_model = Final_Classifier()\n",
        "test_model = test_model.to(device)\n",
        "test_model.weights_init()\n",
        "\n",
        "# compute confusion matrix for seen and unseen classes separately\n",
        "\n",
        "cm_unseen = test_model.compute_confusion_matrix(x_train[7057:], y_train[7057:])\n",
        "cm_seen = test_model.compute_confusion_matrix(x_train[:7057], y_train[:7057])\n",
        "\n",
        "# compute per class accuracy matrix\n",
        "\n",
        "acc_mat_us = ((cm_unseen.diag()/cm_unseen.sum(1)))\n",
        "acc_mat_s = ((cm_seen.diag()/cm_seen.sum(1)))\n",
        "\n",
        "unseen_acc = []\n",
        "seen_acc = []\n",
        "\n",
        "# Remove Nan's from irrelevant classes\n",
        "\n",
        "for acc in acc_mat_us:\n",
        "    if not math.isnan(acc):\n",
        "        unseen_acc.append(acc)\n",
        "\n",
        "for acc in acc_mat_s:\n",
        "    if not math.isnan(acc):\n",
        "        seen_acc.append(acc)\n",
        "\n",
        "# Compute per class accuracy\n",
        "\n",
        "per_class_seen = np.mean(seen_acc)\n",
        "per_class_unseen = np.mean(unseen_acc)\n",
        "l =[per_class_seen, per_class_unseen]\n",
        "\n",
        "print(\"unseen classs accuray for zsl is \", per_class_unseen)\n",
        "print(\"seen classs accuray is \", per_class_seen)\n",
        "print(\"harmonic mean for generalized zsl\", statistics.harmonic_mean(l))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unseen classs accuray for zsl is  0.0126\n",
            "seen classs accuray is  0.00094783306\n",
            "harmonic mean for generalized zsl 0.0017630415902634567\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}